### Mini Description

Learning values directly or indirectly from humans

### Description

It has been argued that it's quite plausible that researchers and developers will want to make agents that act autonomously and powerfully across many domains ([Russell et al. 2015](http://futureoflife.org/data/documents/research_priorities.pdf)). Specifying one's preferences somewhat explicitly in broad or general domains in the style of near-future narrow-domain machine ethics may not be practical, making aligning the values of powerful AI systems with one's own values and preferences difficult ([Soares 2016](https://intelligence.org/files/ValueLearningProblem.pdf), [Soares and Fallenstein 2014a](http://intelligence.org/files/TechnicalAgenda.pdf)). Concretely writing out the full intentions of the operators in a machine-readable format is implausible if not impossible, even for simple tasks ([Soares and Fallenstein 2014a](http://intelligence.org/files/TechnicalAgenda.pdf)). An intelligent agent must be designed to learn and act according to the preferences of its operators, likely with multiple layers of indirection ([Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)).

### Related Nodes

- [Value Interrogation](/Value_Alignment/Validation/Technical_Value_Alignment/Ethics_Mechanisms/Value_Learning/Value_Elicitation/Value_Interrogation/Value_Interrogation.md)
- [Oversight](/Value_Alignment/Control/Oversight/Oversight.md)
- [Human Preference Aggregation](/Value_Alignment/Ethics/Descriptive_Ethics/Human_Preference_Aggregation/Human_Preference_Aggregation.md)
- [Ontological Value](/Value_Alignment/Ethics/Metaethics/Ethical_uncertainty/Metaethical_uncertainty/Ontological_ethical_effects/Ontological_Value/Ontological_Value.md)
