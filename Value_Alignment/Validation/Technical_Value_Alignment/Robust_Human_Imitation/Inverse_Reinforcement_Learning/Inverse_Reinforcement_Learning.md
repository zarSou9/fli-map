### Mini Description

Learning the reward function that a human is approximately optimizing from observing the human's actions

### Description

A system can infer the preferences of another rational, or nearly rational, actor by observing its behavior. A prominent family of algorithms for learning and imitating human behavior, as well as narrow or instrumental values, is inverse reinforcement learning (IRL, [Taylor et al. 2016](https://intelligence.org/files/AlignmentMachineLearning.pdf)). It attempts to learn the reward function that a human is approximately optimizing ([Russell 1998](https://people.eecs.berkeley.edu/~russell/papers/colt98-uncertainty.pdf), [Ng and Russell 2000](https://people.eecs.berkeley.edu/~russell/papers/ml00-irl.pdf)). Very related are apprenticeship learning ([Abbeel and Ng 2004](http://ai.stanford.edu/~ang/papers/icml04-apprentice.pdf), [Abbeel et al. 2010](http://dx.doi.org/10.1177/0278364910371999)) and other methods for estimating a user's reward function ([Ziebart et al. 2008](http://www.aaai.org/Papers/AAAI/2008/AAAI08-227.pdf), [Ratliff et al. 2006](http://martin.zinkevich.org/publications/maximummarginplanning.pdf)). IRL has many variants that improve the generality, aggregability, or analysis of the reward function, or the workflow to approaching it ([Finn et al. 2016a](http://jmlr.org/proceedings/papers/v48/finn16.pdf)). A variant called interactive IRL attempts to learn about the reward function at the same time as trying to maximize it ([Armstrong and Leike 2016](https://dl.dropboxusercontent.com/u/23843264/Permanent/towards-interactive-inverse-reinforcement-learning.pdf)). Issues with the unidentifiability of the reward function might also be addressable by active IRL ([Amin and Singh 2016](https://pdfs.semanticscholar.org/04cf/f669a73c4b6c84124de6e88562cab742c6cb.pdf)). IRL methods might not scale safely, however, due to their reliance on the faulty assumption that human demonstrators are optimizing for a specific reward function, where in reality humans are often irrational, ill-informed, incompetent, and immoral. Recent work has begun to address these issues ([Evans et al. 2015](http://stuhlmueller.org/papers/preferences-nipsworkshop2015.pdf), [Evans et al. 2015a](https://www.aaai.org/Conferences/AAAI/2016/Papers/03Evans12476.pdf)). Generative adversarial imitation learning is another promising approach to this problem ([Ho and Ermon 2016](http://papers.nips.cc/paper/6391-generative-adversarial-imitation-learning.pdf)).

### Related Nodes

- [Narrow Value Learning](/Value_Alignment/Validation/Technical_Value_Alignment/Robust_Human_Imitation/Narrow_Value_Learning/Narrow_Value_Learning.md)
- [Human Judgement Learner](/Value_Alignment/Control/Oversight/Scalable_Oversight/Human_Judgement_Learner/Human_Judgement_Learner.md)
- [Etiquette](/Value_Alignment/Ethics/Descriptive_Ethics/Etiquette/Etiquette.md)
- [Observed choices](/Value_Alignment/Ethics/Descriptive_Ethics/Observed_choices/Observed_choices.md)
